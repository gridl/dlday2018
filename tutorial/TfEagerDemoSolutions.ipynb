{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SOLUTIONS: TfEagerDemo.ipynb","version":"0.3.2","provenance":[{"file_id":"1FWfn81T0hWC3vMLqb_2rR64FSrghwzra","timestamp":1536653806438}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"Q6N3UfEtlPzn","colab_type":"text"},"cell_type":"markdown","source":["This Colab Notebook is loosely inspired by:\n","https://colab.sandbox.google.com/github/titu1994/tf-eager-examples/blob/master/notebooks/02_logistic_regression.ipynb#scrollTo=EExxt0J1kfLK\n","\n","It covers:\n","\n","* Enabling Eager-Mode in Colab/Jupyter notebooks (where cells can be re-run).\n","* Using TensorFlow's interface to \"[Keras](https://en.wikipedia.org/wiki/Keras)\" with TF-Eager to set up and train \n","   a moderate-quality handwritten digit classifier.\n","* Doing batch-generation and training with low-level Python rather than higher-level TensorFlow APIs.\n","* Using trainable models as building blocks of larger trainable models.\n","* Building custom loss-functions.\n","* Using tf.einsum() to good advantage.\n","* \"Inverting\" a model to find an input that produces a given output.\n","\n"]},{"metadata":{"id":"s_PqiTpaOyAZ","colab_type":"text"},"cell_type":"markdown","source":["We start with setting up TensorFlow globally for Eager-Mode execution. We do this as the very first thing in this notebook."]},{"metadata":{"id":"vGV9xUXnlZ7b","colab_type":"code","colab":{},"cellView":"both"},"cell_type":"code","source":["from __future__ import absolute_import, division, print_function\n","import numpy\n","import tensorflow as tf\n","\n","print('TF Version:', tf.__version__)\n","\n","# Enabling eager mode for tensorflow.\n","# This check makes running this cell (and hence enabling tf-eager) idempotent.\n","try:\n","  tf.enable_eager_execution()\n","  print('TF-Eager mode is enabled.')\n","except ValueError as exn:\n","  if tf.executing_eagerly():\n","    print('TF-Eager mode already was enabled.')\n","  elif 'must be called at program startup' in exn.args[0]:\n","    print ('Eager-Mode must be enabled at start-time.\\n'\n","           'Please Restart the Runtime '\n","           '([Runtime] -> [Restart Runtime] or Ctrl-M).')\n","  else:\n","    # Unknown situation, re-raise exception.\n","    raise\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NbV2tv3rSOry","colab_type":"text"},"cell_type":"markdown","source":["We need some other libraries, and also need to do a bit of set-up."]},{"metadata":{"id":"Pb9-JfWrPcgv","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy\n","import os\n","import pprint\n","\n","from matplotlib import pyplot\n","\n","from tensorflow.contrib.eager.python import tfe\n","from tensorflow.python.keras.datasets import mnist\n","\n","try:  # Python2/3 compatibility hack.\n","  xrange(1)\n","except NameError:  # Python3 does not have xrange().\n","  xrange = range\n","\n","# Seed the numpy and tensorflow random number generators to make this Colab\n","# Notebook more reproducible. This generally helps for debugging, and in\n","# particular also helps when resolving why-am-I-seeing-this student questions\n","# that partially depend on chance.\n","tf.set_random_seed(0)\n","numpy.random.seed(0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zl_MQ109POLY","colab_type":"text"},"cell_type":"markdown","source":["Next, we load our Machine-Learning Dataset, tens of thousands of labeled handwritten digits from MNIST.\n","Let us also see how some examples actually look like."]},{"metadata":{"id":"f44IN_4lmqD_","colab_type":"code","colab":{}},"cell_type":"code","source":["# Documentation on this dataset: \n","# https://keras.io/datasets/#mnist-database-of-handwritten-digits\n","(x_train_uint8, y_train_uint8), (x_test_uint8, y_test) = mnist.load_data()\n","x_train = x_train_uint8.astype(numpy.float32) / 255.0\n","x_test = x_test_uint8.astype(numpy.float32) / 255.0\n","y_train = y_train_uint8.astype(numpy.int32)\n","\n","# Let us visualize some of the training-digits.\n","for n in xrange(10):\n","  axes = pyplot.subplot(2, 5, n + 1)\n","  axes.matshow(x_train[n])\n","  axes.grid(False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cP9spMRjSgmt","colab_type":"text"},"cell_type":"markdown","source":["Now, let us set up and train a basic handwritten digit classifier. This implementation shows how this looks like generically, but is (deliberately) not at the state-of-the-art for this problem set.\n","\n","For this example, we do want a classifier that is not quite \"perfect\", since we want to study improving the performance of a somewhat weak model built on top of it."]},{"metadata":{"id":"GHdgedSbvBYR","colab_type":"code","colab":{}},"cell_type":"code","source":["# We define a basic MNIST-classifier model, along the lines of:\n","# https://www.tensorflow.org/guide/eager#train_a_model\n","\n","# This network architecture is not the best choice for this task, but just\n","# an example for how such a network may look like generically. Also,\n","# there are many different ways to set up a trainable deep net. Implementing\n","# a subclass of tf.keras.Model (like here) is just one of them. Our model is\n","# simple enough to make tf.keras.Sequential() a more attractive option.\n","# One may - for example - also use more low-level operations to directly work\n","# with explicit trainable weights- and biases-tensors.\n","class MNISTModel(tf.keras.Model):\n","  def __init__(self):\n","    super(MNISTModel, self).__init__()\n","    self.dense1 = tf.keras.layers.Dense(\n","        units=120,\n","        activation=tf.keras.activations.relu,\n","        kernel_regularizer=tf.keras.regularizers.l2(0.003))\n","    self.dense2 = tf.keras.layers.Dense(\n","        units=40,\n","        activation=tf.keras.activations.relu,\n","        kernel_regularizer=tf.keras.regularizers.l2(0.003))\n","    self.dense3 = tf.keras.layers.Dense(units=10) \n","\n","  def call(self, input):\n","    \"\"\"Runs the model (overridden method).\"\"\"\n","    activations1 = self.dense1(input)\n","    activations2 = self.dense2(activations1)\n","    return self.dense3(activations2)\n","\n","# Get a model-object. This has random-initialized weights.\n","mnist_model = MNISTModel()\n","print('MNIST Model Object:', mnist_model)\n","\n","# We can apply this model straightaway - if we synthesize a batch with\n","# just one example. The model will return a [num_examples, 10]-tensor\n","# which, for each batch-example gives us a logit (think: linearly \n","# additive evidence) for the input to represent the digit '0', the digit '1',\n","# etc.\n","debug_batch0 = tf.constant(x_train[0].reshape((1, 28**2)))\n","debug_eval0 = mnist_model(debug_batch0)\n","# Logits are perhaps not as telling as probabilities, so we map logits\n","# to actual probabilities with tf.nn.softmax(). As the model's initial weights\n","# are essentially random, we expect these probabilities to be very roughly\n","# similar, and not meaningful.\n","print('Example Model Evaluation:', tf.nn.softmax(debug_eval0))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OoxzmPTnTRXx","colab_type":"text"},"cell_type":"markdown","source":["For a basic handwritten digit classifier, the simplest interesting figure of merit is its accuracy: What is the percentage of correctly classified digits, for digits that the model has not seen in its training?"]},{"metadata":{"id":"n7WcEI_f9s88","colab_type":"code","colab":{}},"cell_type":"code","source":["# We define a function that allows us to measure performance of a MNIST \n","# model classifier on the test-set.\n","#\n","# We do this in an ad-hoc way here, relying on TF-Eager being mostly a wrapping\n","# layer around numpy. For later, we add the ability to only measure this on\n","# some stride of the test-set. This is a simple way to split the test-set into\n","# a validation-set and an actual test-set-for-final-evaluation.\n","def mnist_model_test_set_accuracy(model, offset=0, skip=1):\n","  \"\"\"Computes the accuracy of a MNIST-model on the test set.\"\"\"\n","  predicted_logits = model(\n","      tf.constant(x_test.reshape(-1, 28**2)[offset::skip, :])).numpy()\n","  predicted_digits = numpy.argmax(predicted_logits, axis=1)\n","  return numpy.average(predicted_digits == y_test[offset::skip])\n","\n","\n","# The initial model is basically just guessing random answers, so we expect it\n","# to be correct roughly 10% of the time.\n","print('Initial MNIST model accuracy:',\n","      mnist_model_test_set_accuracy(mnist_model))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TZR2g8F4T4ZY","colab_type":"text"},"cell_type":"markdown","source":["Let us train this model in a rather pedestrian way, that is, using only low-level TensorFlow, not higher level APIs."]},{"metadata":{"id":"j8Y2cdRT6Ggl","colab_type":"code","colab":{}},"cell_type":"code","source":["# Training the model. This roughly follows:\n","# https://www.tensorflow.org/guide/eager#train_a_model\n","# There are more high-level ways to do model-training with TensorFlow.\n","# In this tutorial, we tend to mix lower-level and higher-level TensorFlow\n","# functions/methods, to demonstrate how one can do things in different ways.\n","\n","LEARNING_RATE = 1e-4\n","NUM_EPOCHS = 20  # Sufficient for a basic demo. 40 is better.\n","BATCH_SIZE = 128\n","\n","def mnist_loss(model, x, y):\n","  prediction = model(x)\n","  with tf.device(\"/cpu:0\"):  # Device placement example (TF can do this for us).\n","   ret = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=prediction)\n","  return ret\n","\n","\n","def mnist_grad(model, inputs, targets):\n","  tape = tf.GradientTape()\n","  with tape:\n","    loss_value = mnist_loss(model, inputs, targets)\n","  return tape.gradient(loss_value, model.variables)\n","\n","\n","mnist_optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n","\n","\n","# We do all the batching manually here.\n","for num_epoch in xrange(NUM_EPOCHS):\n","  # For better reproducibility, we use a random number generator seeded with\n","  # the number of the epoch.\n","  rng = numpy.random.RandomState(seed=num_epoch)\n","  permutation = numpy.array(range(len(x_train)))\n","  rng.shuffle(permutation)\n","  x_train_perm = x_train[permutation]\n","  y_train_perm = y_train[permutation]\n","  #\n","  num_batch = 0\n","  while (1 + num_batch) * BATCH_SIZE < len(x_train):\n","    batch_slice = slice(num_batch * BATCH_SIZE, (1 + num_batch) * BATCH_SIZE)\n","    x_train_batch = tf.constant(\n","        x_train_perm[batch_slice, :, :].reshape(BATCH_SIZE, 28**2))\n","    y_train_batch = tf.constant(\n","        y_train_perm[batch_slice].reshape(BATCH_SIZE))\n","    gradient = mnist_grad(mnist_model, x_train_batch, y_train_batch)\n","    mnist_optimizer.apply_gradients(zip(gradient, mnist_model.variables))\n","    if num_batch % 200 == 0:\n","      print(\"E={:04d}/{:04d}: Loss={:.6g}, Accuracy(Validation-Set)={:.2f}%\"\n","            .format(num_epoch, num_batch,\n","                    mnist_loss(mnist_model, x_train_batch, y_train_batch),\n","                    100 * mnist_model_test_set_accuracy(mnist_model,\n","                                                        offset=1, skip=2)))\n","    num_batch += 1\n","    \n","print('Done Training. Test-Set Accuracy: {:.2f}%'.format(\n","    100 * mnist_model_test_set_accuracy(mnist_model, offset=0, skip=2)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b5RNkfdUUdMX","colab_type":"text"},"cell_type":"markdown","source":["We now should have a trained model with about 96-98% accuracy, which for this problem is actually pretty low for current standards. Still, this is just right to get a somewhat poor multi-digit classifier which we then want to improve using methods covered in this workshop. Let us plot some predictions to see if they make sense."]},{"metadata":{"id":"hBIWNhtHVGBn","colab_type":"code","colab":{}},"cell_type":"code","source":["# Plotting some example predictions.\n","for n in xrange(12):\n","  axes = pyplot.subplot(2, 6, n + 1)\n","  predicted_logits = mnist_model(\n","      tf.constant(x_test[n].reshape((1, 28**2)))).numpy().reshape((10,))\n","  predicted_number = numpy.argmax(predicted_logits)\n","  axes.matshow(x_test[n])\n","  axes.set_title('T={}, P={}'.format(y_test[n], predicted_number),\n","                 y=0, color='b')\n","  axes.grid(False)\n","  \n","model_logits_testset = mnist_model(x_test.reshape((len(x_test), 28**2)))\n","model_predictions = numpy.argmax(model_logits_testset.numpy().reshape((-1, 10)),\n","                                 axis=1)\n","print('Predictions:', model_predictions[:12], '...')\n","print('Truth:      ', y_test[:12], '...')\n","accuracy = numpy.average([prediction == groundtruth\n","                          for prediction, groundtruth in zip(model_predictions,\n","                                                            y_test)])\n","print('Accuracy: {:7.2f}%'.format(accuracy * 100))\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"6zs1wUggY3Qn","colab_type":"text"},"cell_type":"markdown","source":["Let us now consider a variant of this problem: We want to scan handwritten account numbers. For simplicity, we assume that each digit has been written into its own box, which has been digitized independently, and there are only four digits in total.\n","\n","Let us also assume that these 4-digit account-numbers actually come with a built-in checksum: All valid account numbers are divisible by 9. Having this extra knowledge should somehow be exploitable by ML, so if a handwritten account number could be read as either '2519' or '2619', the model should be able to have learned that '2519' cannot be a correct interpretation.\n","\n","If the probabilities to recognize each single digit correctly were independent, then a single-digit success rate of 96% would mean that the odds of recognizing a 4-digit number correctly is 0.96^4, i.e. about 85%. However, this is not what we would expect in reality, since the difficulty to recognize digits written by the same person is not uncorrelated. For people with very clear handwriting, we would expect to easily recognize just about every 4-digit number they write, while for people with bad handwriting, we would expect a high probability to have two or more barely-recognizable digits in the number.\n","\n","There are many different ways for designing the architecture of such a ML model. Here, we want to do it as follows:\n","\n","\n","*   While we have to recognize four digits, we want these four sub-problems to use the same trainable parameters, so we in effect only train one handwritten-digit-recognizer.\n","*   For each of the 4 digits, we get a 10-dimensional vector of logits (think: accumulated evidence) for the digit to be a 0, 1, 2, etc. We explicitly map these to probabilities.\n","*   We want to have two contributions to the overall loss-function: Cross-entropy loss on the predicted digits as before, plus some extra loss that depends on whether the 4-digit number as it was read is divisible by 9.\n","*   We expect that most problems come from single-digit errors, such as mis-reading a '5' as a '6' in '5112'. While we could try to design a network that could in principle figure out that it needs to have another very close look at the pixel data to determine whether the '2' is really a '2' or rather a '3', or alternatively the '5' is really a '5' or rather a '6' in the number '2510', we want to consider this as beyond our scope. Rather, we want something that only tweaks the predicted likelihoods for the 1st digit to be a 0, a 1, a 2, etc. according to what the other digits likely are - and correspondingly for the 2nd digit, etc.\n","*  Strictly speaking, we should have a logit for each possible output-number from '0000' to '9999'. Nevertheless, we use an architecture where we always make the crude assumption that the probability for a given number is the product of the probabilities for all the digits, and the 'divisible-by-9' constraint is implemented as adjustments to single-digit probabilities found by looking at the probability-distribution for all the digits.\n","*   So, after we obtained the 4x10 logits for the likelihoods for the thousands, hundreds, tens, and ones to be a particular digit, we want to have some other layer(s) that predict corrections to these logits. In training this correction-predicting part, we add a loss that punishes predicting numbers not divisible by 9.\n","\n","This approach may be neither the smartest nor the simplest thing to do for such a problem. The simplest approach likely would be to pick the 4-digit number with highest predicted likelihood that is also divisible by 9. Still, we want to do this as sketched above, because this will give us a good tour of a number of useful techniques.\n","\n"]},{"metadata":{"id":"j1zdr7q4a4Ps","colab_type":"text"},"cell_type":"markdown","source":["Let us first implement a component that tells us, for four 10-vectors of logits, the probability for the final number to be divisible by 9. We keep this very low-level to demonstrate how to mix higher-level concepts like Keras models or layers with low-level TensorFlow."]},{"metadata":{"id":"GRTsFT8nbPJE","colab_type":"code","colab":{}},"cell_type":"code","source":["# We use an auxiliary tensor constant which, for two digits, tells us the mod-9\n","# remainder of their sum.\n","# It is generally not helpful to think of a rank-3 tensor like this as some\n","# sort of 'three-dimensional matrix' (even more so for ranks beyond 3).\n","# Tensors like this are better interpreted as some sort of SQL-like table,\n","# which in this particular case would roughly look as follows (listing only\n","# non-zero entries):\n","#\n","# Summand1Mod9,     Summand2Mod9,      SumMod9, Multiplier\n","# 0,                0,                 0,       1\n","# 0,                1,                 1,       1\n","# 0,                2,                 2,       1\n","# (...)\n","# 1,                0,                 1,       1\n","# 1,                1,                 2,       1\n","# (...)\n","# 2,                4,                 6,       1\n","# (...)\n","# 5,                7,                 3,       1\n","# (...)\n","mod9sum = numpy.zeros([10, 10, 10], dtype=numpy.float32)\n","for n1 in xrange(10):\n","  for n2 in xrange(10):\n","    mod9sum[n1, n2, (n1 + n2) % 9] = 1.0\n","\n","# The corresponding tensor-constant:\n","tc_mod9sum = tf.constant(mod9sum)\n","\n","# Index 'b' is the batch-example index. Number of batch-examples is 'B'.\n","def get_prob_for_being_multiple_of_9(logit_by_digit_and_place):\n","  # The parameter 'logit_by_digit_and_place' is a (B, 10, 4)-tensor.\n","  # Entry [b, 7, 0] is the predicted logit-space evidence for the 4-digit\n","  # number to have a '7' in the 'thousands' place, for example 'b'.\n","  # Entry [9, 1] is the predicted logit-space evidence for the 4-digit\n","  # number to have a '9' in the 'hundreds' place, for example 'b'.\n","  #\n","  # We first map logits to probabilities with softmax().\n","  probability_by_digit_and_place = tf.nn.softmax(logit_by_digit_and_place)\n","  # Index-names correspond to roman-numeral place-values.\n","  #\n","  #\n","  # Here and below: We use latin-inspired index-names M, C, X, I for thousands,\n","  # hundreds, tens, and ones. The 'tf.einsum()' tensor-product can be read as an \n","  # equivalent SQL statement roughly of the form:\n","  #\n","  # select M.NumExample, M.Digit, C.Digit, M.Value * C.Value\n","  # from ExamplesThousands as M, ExamplesHundreds as C\n","  # where M.NumExample = C.NumExample;\n","  #\n","  # (B, 10, 10)-tensor.\n","  # Entry [b, 2, 3] is the probability for the number to be of the form 23xx.  \n","  prob_digits12 = tf.einsum('bM,bC->bMC',\n","                            probability_by_digit_and_place[:, 0],\n","                            probability_by_digit_and_place[:, 1])\n","  # (B, 10)-tensor.\n","  # Entry [b, 7] is the probability for the sum-mod-9 of the first 2 digits\n","  # to be 7.\n","  mod9prob_digits12 = tf.einsum('MCr,bMC->br', tc_mod9sum, prob_digits12)\n","  # Likewise: Digits 3 and 4.\n","  prob_digits34 = tf.einsum('bX,bI->bXI',\n","                            probability_by_digit_and_place[:, 2],\n","                            probability_by_digit_and_place[:, 3])\n","  mod9prob_digits34 = tf.einsum('XIs,bXI->bs', tc_mod9sum, prob_digits34)\n","  # (B, 10, 10)-tensor. Entry [b, 4, 7] is the probability for the first\n","  # two digits to have mod-9 sum 4 and the last two digits to have mod-9 sum 7.\n","  mod9prob_digits12_34 = tf.einsum('br,bs->brs',\n","                                   mod9prob_digits12,\n","                                   mod9prob_digits34)\n","  # (B, 10)-tensor. Entry [b, 5] is the probability for the number to have\n","  # mod-9 digit-sum 5.\n","  # TASK: Simplify the code so that we only compute entry 0.\n","  mod9prob_all = tf.einsum('rst,brs->bt', tc_mod9sum, mod9prob_digits12_34)\n","  # Return probability for mod-9 sum to be 0 (for all batch-examples).\n","  return mod9prob_all[:, 0]\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ad-AiKpyp2nS","colab_type":"text"},"cell_type":"markdown","source":["This plausibility check shows what this function does. Note that due to TF-Eager, we can work with TF-tensors pretty much as if they were numpy arrays."]},{"metadata":{"id":"Awvb2Li7auAl","colab_type":"code","colab":{}},"cell_type":"code","source":["# Some artificial logits that correspond to a 4-digit number that most likely\n","# would read as '0008'.\n","t_ddd_ex = tf.constant(numpy.array([\n","    [ 9.0, -1.0, -1.0, -1.0, -1.0,  -1.0, -1.0, -1.0, -1.0, -1.0],\n","    [ 9.0, -1.0, -1.0, -1.0, -1.0,  -1.0, -1.0, -1.0, -1.0, -1.0],\n","    [ 9.0, -1.0, -1.0, -1.0, -1.0,  -1.0, -1.0, -1.0, -1.0, -1.0],\n","    [-1.0,  -1.0, -1.0, -1.0, -1.0,  -1.0, -1.0, -1.0, 9.0, -1.0]],\n","    dtype=numpy.float32))\n","\n","print('Probability',\n","      get_prob_for_being_multiple_of_9(tf.reshape(t_ddd_ex, (1, 4, 10))))\n","\n","# EXERCISE: Add another example with a logit-distribution that corresponds to\n","# likely seeing a number that is divisible by 9.\n","\n","# SOLUTION: (There are many options here.)\n","t_ddd_ex1809 = tf.constant(numpy.array([\n","    [-1.0, 20.0, -1.0, -1.0, -1.0,  -1.0, -1.0, -1.0, -1.0, -1.0],\n","    [ -5.0, -1.0, -1.0, -1.0, -1.0,  -1.0, -1.0, -1.0, 20.0, -1.0],\n","    [ 9.0, -1.0, -1.0, -1.0, -1.0,  -1.0, -1.0, -1.0, -1.0, -1.0],\n","    [-1.0,  -1.0, -1.0, -1.0, -1.0,  -1.0, -1.0, -1.0, -1.0, 50.0]],\n","    dtype=numpy.float32))\n","print('Probability for a number likely reading as 1809:',\n","      get_prob_for_being_multiple_of_9(tf.reshape(t_ddd_ex1809, (1, 4, 10)))\n","      .numpy()[0])\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_SNi64Ntax2c","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"DCJC4sWpWuNb","colab_type":"text"},"cell_type":"markdown","source":["Next, let us build a simple ML model for reading a 4-digit account number. The basic idea is that we apply our single-digit recognition model to each of the four digits, but then take the logits and feed them through two extra layers that can tweak them.\n","\n","In order for this to make sense, we will need to provide some criterion for what a good way to tweak the prediction looks like. Here, this criterion will be the probability for the predicted 4-digit number to be divisible by 9. This is a custom loss function for this particular design approach to the problem.\n","\n","What do we expect this to achieve? In a situation where the four single-digit-recognition problems left us with e.g. \"pretty sure that the first 3 digits are 2, 3, and 1, but for the last digit, there is a 50-50 chance that this is a 2 or a 3\", the divisible-by-9 rule tells us that we should predict a 3 (so we can boost that logit).\n","\n","Note that if we have a 96% chance to get a single digit read correctly, then running the same model on four independent digits gives us a probability of 100% * 0.96^4 = ca. 85% to get all four digits right. Our initial 4-digit model, that can apply tweaks, starts out as not being trained to apply such changes correctly, so we expect worse-than-this-baseline behavior."]},{"metadata":{"id":"xJNgxUpFWl9z","colab_type":"code","colab":{}},"cell_type":"code","source":["# For the four-digit-account-number model, we allow plugging in a separately \n","# trained digit-recognition model, and allow the user to specify whether\n","# training should tweak that model or not.\n","\n","class AccountNumberModel(tf.keras.Model):\n","  def __init__(self, mnist_model, train_mnist_model=False):\n","    super(AccountNumberModel, self).__init__()\n","    self._mnist_model = mnist_model\n","    self._train_mnist_model = train_mnist_model\n","    # As for the MNIST-model, this architecture does not try to be\n","    # especially clever.\n","    self._dense1 = tf.keras.layers.Dense(\n","        units=120,\n","        activation=tf.keras.activations.relu,\n","        kernel_regularizer=tf.keras.regularizers.l2(0.03))\n","    # This layer is supposed to return a correction to be added to 4*10 logits,\n","    # so needs to have 4*10 units.\n","    self._dense2 = tf.keras.layers.Dense(\n","        units=4 * 10,\n","        activation=tf.keras.activations.relu,\n","        kernel_regularizer=tf.keras.regularizers.l2(0.03))\n","    \n","\n","  def _apply_mnist_model(self, single_digit_input):\n","    # If the model was set up to allow training the contained MNIST-model.\n","    # we allow gradient-backpropagation to continue into the weights of\n","    # this sub-model. Otherwise, we stop gradient-backpropagation here.\n","    if self._train_mnist_model:\n","      return self._mnist_model(single_digit_input)\n","    else:\n","      return tf.stop_gradient(self._mnist_model(single_digit_input))\n","    \n","  def call(self, input):\n","    \"\"\"Runs the model (overridden method).\"\"\"\n","    pixeldata_by_digit = tf.reshape(input, (-1, 4, 28**2))\n","    # (B, 4, 10)-Tensor. Entry (b, 0, 6) is the logit given by the MNIST-model\n","    # for the leading (thousands)-digit of the account number to be '6'.\n","    raw_logit_by_place_and_digit = tf.stack(\n","        [self._apply_mnist_model(pixeldata_by_digit[:, num_place, :])\n","         for num_place in (0, 1, 2, 3)], axis=1)\n","    d1 = self._dense1(tf.reshape(raw_logit_by_place_and_digit, (-1, 40)))\n","    d2 = self._dense2(d1)\n","    correction = tf.reshape(d2, (-1, 4, 10))\n","    return raw_logit_by_place_and_digit + correction \n","\n","\n","acct_model = AccountNumberModel(mnist_model, train_mnist_model=False)\n","\n","\n","mnist_model_accuracy_before_acct_model_training = (\n","    mnist_model_test_set_accuracy(mnist_model, offset=0, skip=2))\n","\n","print('MNIST Model Accuracy: {:.2f}% - Baseline Account-Model Accuracy: {:.2f}%'\n","      .format(100 * mnist_model_accuracy_before_acct_model_training,\n","              100 * mnist_model_accuracy_before_acct_model_training**4))\n","\n","\n","# Trying out the account-number model on 'meaningless' input data\n","# ('four empty boxes'), just to ensure that it can be applied.\n","debug_acct0 = acct_model(tf.constant(numpy.zeros([1, 4, 28**2],\n","                                                 dtype=numpy.float32)))\n","print('debug_acct0', debug_acct0)  # This shows raw logits, not probabilities.\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GK2gA2loZV-G","colab_type":"text"},"cell_type":"markdown","source":["Let us train this model. Here, we do not actually use a conventional batched training set. Rather, we keep randomly assembling fresh batches of 4-digit numbers from the MNIST examples."]},{"metadata":{"id":"N0uGrs8xdVmp","colab_type":"code","colab":{}},"cell_type":"code","source":["ACCT_LEARNING_RATE = 1e-4\n","ACCT_BATCH_SIZE = 128\n","ACCT_NUM_ITERATIONS = 4000\n","ACCT_MOD9_LOSS_WEIGHT = 1e3\n","\n","\n","def fourdigits(ds):\n","  '''Helper function. Formats 4-vector of digits as digit-string.'''\n","  return '{}{}{}{}'.format(*ds)\n","\n","\n","def acct_loss(acct_model, x, y):\n","  prediction = acct_model(x)\n","  # This shows how to control hardware-association in TensorFlow.\n","  # It is actually not necessary here, since TensorFlow knows where it\n","  # can place tf.losses.sparse_softmax_cross_entropy() and where it can not.\n","  # with tf.device(\"/cpu:0\"):\n","  digits_loss = tf.losses.sparse_softmax_cross_entropy(\n","       labels=y, logits=prediction)\n","  mod9_loss = 1.0 - get_prob_for_being_multiple_of_9(prediction)\n","  return tf.reduce_mean(digits_loss + ACCT_MOD9_LOSS_WEIGHT * mod9_loss)\n","\n","\n","def acct_grad(acct_model, inputs, targets):\n","  tape = tf.GradientTape()\n","  with tape:\n","    loss_value = acct_loss(acct_model, inputs, targets)\n","  return tape.gradient(loss_value, acct_model.variables)\n","\n","\n","acct_optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n","\n","# When generating example batches of handwritten 4-digit numbers,\n","# the final digit is a function of the first three digits.\n","# We hence want to know, for each digit, what the candidates are.\n","candidates_by_digit = [[] for _ in xrange(10)]\n","for n, digit in enumerate(y_train):\n","  candidates_by_digit[digit].append(n)\n","\n","candidate_acct_numbers = numpy.array([[x // 1000 % 10,\n","                                       x // 100 % 10,\n","                                       x // 10 % 10,\n","                                       x % 10] for x in xrange(0, 10000, 9)])\n","\n","\n","# Here, we do training in a more ad-hoc fashion, and randomly generate each\n","# batch for batch-gradient-descent by first drawing some valid account-numbers\n","# at random, and then finding random instances of handwritten digits.\n","def generate_acct_training_batch(batch_size=ACCT_BATCH_SIZE, seed=0):\n","  '''Synthesizes an account-model training-batch.'''\n","  acct_rng = numpy.random.RandomState(seed=seed)\n","  batch_acct_numbers = candidate_acct_numbers[\n","    acct_rng.randint(0, len(candidate_acct_numbers), batch_size)]\n","  choices = numpy.array(\n","      [[candidates_by_digit[d][acct_rng.randint(0, len(candidates_by_digit[d]))]\n","        for d in digits]\n","       for digits in batch_acct_numbers], dtype=int)\n","  raw_account_numbers = y_train[choices]\n","  y_train_batch = tf.constant(y_train[choices])\n","  x_train_batch = tf.constant(x_train[choices])\n","  return x_train_batch, y_train_batch\n","\n","\n","# We generate one particular batch which we hold on to. We will repeatedly\n","# evaluate and report performance on this particular random batch.\n","acct_batch0_x, acct_batch0_y = generate_acct_training_batch(batch_size=10000,\n","                                                            seed=10**8)\n","\n","def get_acct_model_batch0_performance(model):\n","  '''Computes performance of an account-model on acct_batch0_(x,y).\n","  \n","  Args:\n","    model: an AccountNumberModel model.\n","    \n","  Returns:\n","    Pair of (accuracy, diffs), where `accuracy` reports the model-accuracy,\n","    and `diffs` is a list of triplets\n","    (example_index, 4-digit-string prediction, 4-digit-string target)\n","    listing the deviations between predictions and targets.\n","  '''\n","  predictions = numpy.argmax(model(acct_batch0_x)[:,:,:].numpy(), axis=-1)\n","  targets = acct_batch0_y.numpy()\n","  diffs = [(n, fourdigits(p), fourdigits(t))\n","           for n, (p, t) in enumerate(zip(predictions, targets))\n","           if tuple(p) != tuple(t)]\n","  accuracy = 1.0 - len(diffs) / float(len(targets))\n","  return accuracy, diffs\n","\n","\n","for num_iteration in xrange(ACCT_NUM_ITERATIONS):\n","  x_train_batch, y_train_batch = generate_acct_training_batch(\n","      seed=num_iteration)\n","  acct_gradient = acct_grad(acct_model, x_train_batch, y_train_batch)\n","  # Note that acct_model.variables will include the MNIST-model's variables,\n","  # if we allowed gradient-backpropagation into the MNIST model.\n","  acct_optimizer.apply_gradients(zip(acct_gradient, acct_model.variables))\n","  if num_iteration % 100 == 0:\n","    accuracy, diffs = get_acct_model_batch0_performance(acct_model)\n","    print(\"N={:06d}: Loss={:.6g}, Accuracy(batch0)={:.2f}%, diffs: {}\".format(\n","          num_iteration,\n","          acct_loss(acct_model, x_train_batch, y_train_batch),\n","          100 * accuracy, diffs[:3]))\n","\n","(acct_model_final_accuracy, _) = get_acct_model_batch0_performance(acct_model)\n","mnist_model_accuracy_after_acct_model_training = (\n","    mnist_model_test_set_accuracy(mnist_model, offset=0, skip=2))\n","print('Done Training Account-Number Model.\\n\\n'\n","      'AC-Model accuracy: {:.2f}%,\\n'\n","      'MNIST accuracy: {:.2f}%,\\n'\n","      'MNIST-only expected accuracy: {:.2f}'.format(\n","          100 * acct_model_final_accuracy,\n","          100 * mnist_model_accuracy_after_acct_model_training,\n","          100 * mnist_model_accuracy_after_acct_model_training**4))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6xYkbfKtcDVr","colab_type":"text"},"cell_type":"markdown","source":["The model which we trained showes somewhat better than baseline-performance: It is better than what we would expect from just running our MNIST-classifier independently on four randomly picked numbers.  So, the model seems to have managed to partially utilize the extra information from the checksum.  Note however, that with this design, the account-number model does actually often predict invalid account-numbers.\n"]},{"metadata":{"id":"80vjdQZ7dGYx","colab_type":"code","colab":{}},"cell_type":"code","source":["# Exercise: Evaluate Account-Number model accuracy on test-set.\n","\n","# SOLUTION (mostly cut&paste).\n","test_candidates_by_digit = [[] for _ in xrange(10)]\n","for n, digit in enumerate(y_test):\n","  test_candidates_by_digit[digit].append(n)\n","\n","# It is actually somewhat tricky here to define what a \"test example\" should be.\n","# Do we allow re-using numbers from the MNIST test-set, or should each digit\n","# only be used once? If there are very many digits, this should not matter much.\n","def generate_acct_test_batch(batch_size=ACCT_BATCH_SIZE, seed=0):\n","  '''Synthesizes an account-model training-batch.'''\n","  acct_rng = numpy.random.RandomState(seed=seed)\n","  batch_acct_numbers = candidate_acct_numbers[\n","    acct_rng.randint(0, len(candidate_acct_numbers), batch_size)]\n","  choices = numpy.array(\n","      [[test_candidates_by_digit[d][\n","        acct_rng.randint(0, len(test_candidates_by_digit[d]))]\n","        for d in digits]\n","       for digits in batch_acct_numbers], dtype=int)\n","  raw_account_numbers = y_test[choices]\n","  y_test_batch = tf.constant(y_test[choices])\n","  x_test_batch = tf.constant(x_test[choices])\n","  return x_test_batch, y_test_batch\n","\n","\n","acct_test_x, acct_test_y = generate_acct_test_batch(batch_size=10000,\n","                                                    seed=7)\n","\n","def get_acct_model_test_performance(model):\n","  '''Computes performance of an account-model on acct_batch0_(x,y).\n","  \n","  Args:\n","    model: an AccountNumberModel model.\n","    \n","  Returns:\n","    Pair of (accuracy, diffs), where `accuracy` reports the model-accuracy,\n","    and `diffs` is a list of triplets\n","    (example_index, 4-digit-string prediction, 4-digit-string target)\n","    listing the deviations between predictions and targets.\n","  '''\n","  predictions = numpy.argmax(model(acct_test_x)[:,:,:].numpy(), axis=-1)\n","  targets = acct_test_y.numpy()\n","  diffs = [(n, fourdigits(p), fourdigits(t))\n","           for n, (p, t) in enumerate(zip(predictions, targets))\n","           if tuple(p) != tuple(t)]\n","  accuracy = 1.0 - len(diffs) / float(len(targets))\n","  return accuracy, diffs\n","\n","(acct_model_test_accuracy, _) = get_acct_model_test_performance(acct_model)\n","print('AC-Model Test-Set Accuracy:', acct_model_test_accuracy)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-GgwVrPubiWK","colab_type":"text"},"cell_type":"markdown","source":["Finally, let us see how we can use TF-Eager's gradient-tape to compute gradients. As an example application, we want to use gradient-descent to adjust an input-vector to our MNIST-model such that it gives some particular prediction: \"Find me an input that gets classified as the number >> 5 <<\"."]},{"metadata":{"id":"sbGXDvufeVSX","colab_type":"code","colab":{}},"cell_type":"code","source":["# 'Inverting' the MNIST-Model: let us use gradient-updates to find a\n","# random-generic image that produces a given classification.\n","#\n","# A problem here is that pixel-values are constrained to [0..1], while\n","# gradient-updates may take us out of that range.\n","# We use the a sigmoid to fix this.\n","noisy_digit_pixel_logits = tf.random_normal([1, 28**2])\n","noisy_digit_pixels = tf.sigmoid(noisy_digit_pixel_logits)\n","print('DDD noisy', tf.nn.softmax(mnist_model(noisy_digit_pixels)))\n","                                \n","# Let us find something that gets classified as some specific number.\n","target_classification = tf.constant([7], dtype=tf.int32)\n","\n","  \n","def invmnist_loss(model, digit_pixel_logits, target):\n","  prediction = model(tf.sigmoid(digit_pixel_logits))\n","  with tf.device(\"/cpu:0\"):\n","   ret = tf.losses.sparse_softmax_cross_entropy(labels=target,\n","                                                logits=prediction)\n","  return ret\n","\n","\n","def invmnist_grad(model, inputs, targets):\n","  tape = tf.GradientTape()\n","  with tape:\n","    # We explicitly have to 'watch' the inputs, since the gradient-tape\n","    # by default only watches parameters marked as 'trainable'.\n","    tape.watch(inputs)\n","    loss_value = invmnist_loss(model, inputs, targets)\n","  # Here, we keep the model's parameters fixed and extract the \n","  # gradient on the input.\n","  return tape.gradient(loss_value, inputs)\n","\n","\n","for n in range(5000):\n","   gradient = invmnist_grad(mnist_model,\n","                            noisy_digit_pixel_logits,\n","                            target_classification)\n","   noisy_digit_pixel_logits -= 0.01 * gradient  # Negative-gradient direction!\n","   if n % 1000 == 0:\n","      print('N={:4d}, L={:8.3f}: {}'.format(\n","          n,\n","          invmnist_loss(mnist_model, \n","                        noisy_digit_pixel_logits, \n","                        target_classification),\n","          ['{:.3f}'.format(x)\n","           for x in tf.nn.softmax(mnist_model(\n","               tf.sigmoid(noisy_digit_pixel_logits))).numpy().reshape(-1)]))\n","\n","axes = pyplot.subplot()\n","axes.matshow(tf.sigmoid(noisy_digit_pixel_logits).numpy().reshape([28, 28]))\n","\n","# Does this strange image really get classified as claimed?\n","pprint.pprint(\n","    zip(range(10),\n","        list(tf.nn.softmax(\n","            mnist_model(\n","                tf.sigmoid(noisy_digit_pixel_logits))).numpy().reshape(-1))))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lrumsbQoA91v","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"kwDd77WEcHYy","colab_type":"code","colab":{}},"cell_type":"code","source":["# EXERCISE: Change the above example to produce an input that gets classified as a '7'.\n","# SOLUTION: We only need to change this number above:\n","#\n","# target_classification = tf.constant([5], dtype=tf.int32)\n","# to:\n","# target_classification = tf.constant([7], dtype=tf.int32)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Tl1IrlM-AcU-","colab_type":"text"},"cell_type":"markdown","source":["This then typically looks as follows:\n","\n","![A typical result](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl0Tff6x/FHo0oEMSS0aixKi7otXUINQVX09hqqFxFROhgui6Jm6pZWhVLDbU1FW7eVCsVd1bJUDVVi6sQ1F9WUCFVj9Bry+6NLlnPO3iefk0Yifu/XX87zfTzffbLjsXN2vvubJy0tLc0AAH7dkdMHAAC5Ac0SAAQ0SwAQ0CwBQECzBAABzRIABHlzYtLXX3/dvvvuO8uTJ48NGzbMatasmROHkaUSExOtb9++VrlyZTMzq1Klio0cOTKHjyrz9u3bZ7169bJnn33WYmJi7NixYzZo0CC7evWqhYWF2YQJEyxfvnw5fZgB8X5PQ4YMsV27dlloaKiZmT333HPWuHHjnD3IAMXFxdn27dvtypUr1r17d6tRo0auP09mvu9rzZo1OX6usr1ZbtmyxY4cOWLx8fF28OBBGzZsmMXHx2f3YdwUjz76qE2dOjWnD+NPu3jxoo0ZM8YiIiLSY1OnTrXo6GiLioqySZMmWUJCgkVHR+fgUQbG6T2ZmfXv398iIyNz6Kj+nM2bN9v+/fstPj7eTp8+bW3atLGIiIhcfZ7MnN9X3bp1c/xcZfuP4Zs2bbJmzZqZmdl9991nZ86csfPnz2f3YcCPfPny2ezZsy08PDw9lpiYaE2bNjUzs8jISNu0aVNOHV6mOL2n3K5OnTo2ZcoUMzMrXLiwpaam5vrzZOb8vq5evZrDR5UDzfLkyZNWtGjR9NfFihWzlJSU7D6Mm+LAgQPWo0cP69ixo23cuDGnDyfT8ubNa/nz5/eIpaampv84V7x48Vx3zpzek5nZggULLDY21l566SX79ddfc+DIMi8oKMiCg4PNzCwhIcEaNmyY68+TmfP7CgoKyvFzlSOfWd7odlltWb58eevdu7dFRUXZ0aNHLTY21latWpUrPy/KyO1yzlq1amWhoaFWrVo1mzVrlk2fPt1GjRqV04cVsNWrV1tCQoLNnTvXmjdvnh7P7efpxve1c+fOHD9X2X5lGR4ebidPnkx/feLECQsLC8vuw8hyJUuWtJYtW1qePHmsbNmyVqJECUtOTs7pw8oywcHBdunSJTMzS05Ovi1+nI2IiLBq1aqZmVmTJk1s3759OXxEgduwYYPNmDHDZs+ebYUKFbptzpP3+7oVzlW2N8v69evbypUrzcxs165dFh4ebiEhIdl9GFlu+fLl9u6775qZWUpKip06dcpKliyZw0eVderVq5d+3latWmUNGjTI4SP68/r06WNHjx41sz8+k73+mwy5xblz5ywuLs5mzpyZfpf4djhPTu/rVjhXeXLiqUMTJ060bdu2WZ48eeyVV16xqlWrZvchZLnz58/bwIED7ezZs3b58mXr3bu3NWrUKKcPK1N27txp48ePt6SkJMubN6+VLFnSJk6caEOGDLHff//d7rnnHhs3bpzdeeedOX2oMqf3FBMTY7NmzbICBQpYcHCwjRs3zooXL57ThyqLj4+3adOmWYUKFdJjb7zxho0YMSLXnicz5/fVtm1bW7BgQY6eqxxplgCQ27CCBwAENEsAENAsAUBAswQAAc0SAAQ0SwAQ0CwBQECzBABBtjxI48anDN3o66+/tnr16nnE1Ad6tmvXTp6/QIECcm65cuWkvPvuu88xXqhQITt37pxH7LPPPpPnV1dbPPTQQ3LNHj16yLkfffSRY7xYsWIeT3q5/hgwRWxsrJx78eJFKS+QZzS+9tprjvGxY8faiBEjPGInTpyQag4bNkyev3z58nJuIM8TGDNmjE9sypQp1rdvX49YIOcqkKdlDR06VMoLZMll27ZtHeNO58r7fboJ5Bmz8+bNcx3L0SvL6wvjbydBQUE5fQg3Rd68Of6Aqix377335vQhZDn1P/vc5lY4V5n+F3A7bg0BAG4y1Sxv560hAMBJpn4MZ2sIAP/fZOqpQyNHjrRGjRqlN8zo6Gh77bXXPB6pdKPdu3fflp9PAvj/I0s+tc+o33rf8b7u9OnTPnfKc/vd8NDQUPvtt988YrfD3fDw8HCPO8W3w93w+fPn27PPPusRy+13w5cuXWqtW7f2iN0Od8OdzlWuuBt+u24NAQBuMtUsb9etIQDATaZ+DH/44YftwQcftA4dOqRvDQEAt7NMf2Y5cODArDwOALilZcuyjAEDBshj69atk2o+/fTT8vzeyw/9+fHHH6W8xx9/3DG+detWnzHvD9z96dq1q5R36tQpuWa3bt3k3Oub22c0VrduXbmm2w0+J5GRkVJeID/N1KlTRx67cOGCVDOQlTLXdyhU3LikNCNuNzi844HsXR/Ivyu17pUrV+SaXbp0kcemTZsm1QzkPfnDgzQAQECzBAABzRIABDRLABDQLAFAQLMEAAHNEgAENEsAENAsAUCQLSt47r//fnlMXe3i/bgmf9THnpmZzZ07V8pbsmSJPDZr1ix5fnUFkb+vqbcSJUrIuceOHXOMV65c2WMskOWugTx2bMiQIVJeII/d2rZtm2P8H//4hyUmJnrEXn31Vanm/Pnz5fkDeURgIOd13759PrHIyEjbsWOHRyyQLV8CWe0zbtw4KW/v3r1yzWvXrrmOVaxY0eP1mjVrpJq1atWS5/eHK0sAENAsAUBAswQAAc0SAAQ0SwAQ0CwBQECzBAABzRIABDRLABDQLAFAkCctLS3tZk/itCzLzKxKlSquYxmZMWOGnNusWTM5121pnLeePXs6xsPCwiwlJcUjFshyS9X48ePl3BdeeEHOLVmypGO8YMGCHpt5nTlzRq65e/duObd+/fpSXiCbsLltWPbSSy/Z5MmTPWJRUVFyXVVqaqqcG8jX1Wlp5N133+2zZHX//v1yzcOHD8u56vf1Tz/9JNcMCwtzjHfr1s1nKXLt2rWlmv6WJnsbPXq06xhXlgAgoFkCgIBmCQACmiUACGiWACCgWQKAgGYJAAKaJQAIaJYAIKBZAoAgW3Z33Lp1q2O8SpUqPmPqTnR33323PP+pU6fkXLdljN7uuMP9/xnvsYsXL8rzq0vIOnXqJNf85ptv5NzmzZu7jt24814gy1Q/+eQTOXfp0qVS3syZM+WaixYtch0rUqSIx+stW7ZINXft2iXPn5CQIOeuX79ezv3yyy99YtHR0T7xs2fPyjUXLFgg57otI/X2xBNPyDU3b97sOua9bHLTpk1SzUB2IvWHK0sAENAsAUBAswQAAc0SAAQ0SwAQ0CwBQECzBAABzRIABDRLABBky4ZlRYsWdYyfPn3aZ0xdQZE/f355/htXnmREnf/jjz92jC9atMieeeYZj1hkZKQ8v9vXylsgG7YtXrxYzp03b55j/OWXX7YJEyakv3bb2MxJIBtmqSuoKlWqJNfcu3evY7xMmTJ29OhRj9iGDRukmidOnJDnb9y4sZzrtAmZm99++80n5rRh2ZtvvinXDORYmzZtKuWdPHlSrjlq1CjH+Lx586xr164esRYtWkg1x44dK8//ww8/uI5xZQkAgkytDU9MTLS+ffta5cqVzeyPNd4jR47M0gMDgFtJph+k8eijj9rUqVOz8lgA4JbFj+EAIMh0szxw4ID16NHDOnbsaBs3bszKYwKAW06m7oYnJyfb9u3bLSoqyo4ePWqxsbG2atUqy5cvn2P+7t27rVq1an/6YAEgp2TJrw61a9fOJk+ebGXKlHEc51eH+NUhFb86xK8OOcm1vzq0fPlye/fdd83MLCUlxU6dOhXQPx4AyG0ydTe8SZMmNnDgQPviiy/s8uXLNnr0aNcfwQHgdpCpZhkSEhLQj4EAkNvdcssd1cOJj4+X5y9QoICc+8gjj0h5UVFRjvH169dbw4YNPWIFCxaU59+zZ4+UF8h7GjFihJzrtmHaM88847HxV/369eWabdu2lXPff/99KS+Qb9vSpUs7xkNCQuz8+fMesf/+979SzStXrvzp+Z1MmjRJzh06dKhPrFSpUnb8+HGPWN26deWa3377rZw7efJkKa9bt25yzVq1ajnGnXrF5cuXpZqBbNh3faGNE37PEgAENEsAENAsAUBAswQAAc0SAAQ0SwAQ0CwBQECzBAABzRIABDRLABBkeluJQHz99dfy2Nq1a6WaISEh8vwxMTFy7sKFC6W8pUuXymOBLGFr3769lLd+/Xq5ZvHixeXcpKQk17Fz586l/1l9lJyZWY8ePeTc1atXS3mBPHbP3/n3Xrbo9ug9b+rjyczM7rhDvyZ58cUX5VynZYQrVqzwiT/22GNyzQEDBsi5Xbp0kfLeeustuWaHDh3ksVatWkk1k5OT5flZ7ggAfxLNEgAENEsAENAsAUBAswQAAc0SAAQ0SwAQ0CwBQECzBABBtqzg8benuPeYuoKnSJEi8vyBbNj1/fffS3k1atRwHcufP7/H6++++06eX92Ia/r06XLNmjVryrmFChVyjHfr1s2mTZuW/tptYyknKSkpcm54eLiUV6lSJbmmv42tvMfWrFkj1YyIiJDnv/HrlpFANjc7ceKEFH/nnXduyvzjxo2T8lq3bi3X9Pfvunv37h6v/a22uVEgm+v527CNK0sAENAsAUBAswQAAc0SAAQ0SwAQ0CwBQECzBAABzRIABDRLABDQLAFAkCdNXV/3J9x3332O8YMHD/qM9ezZU6p58OBBef4KFSrIuYmJiVKe23uKi4uzQYMGecT69esnz3/kyBEpz9+yLG+HDh2Sc902F6tYsaL9+OOP6a/btWsn1/zrX/8q554/f17KU8+Tmfu5ev/99y02NtYjpi4NffXVV+X5P/30Uzn3kUcekXP79OnjE3v33Xftueee84ipSzjNzEqUKCHnHj16VMqLjIyUa7pthPf2229br169PGJXr16Vagbynl577TXXMa4sAUBAswQAAc0SAAQ0SwAQ0CwBQECzBAABzRIABDRLABDQLAFAQLMEAEG27O64fft2eaxatWpSzbCwMHn+sWPHyrk///yzlDdq1Ch57NSpU/L83bp1k/ICWcK5Z88eOfe9995zjCcnJ3vsaOi01M7N4sWL5dyZM2dKeVWqVJFrvv76665j3ruJPv/881LNQJYQqjXNzNavXy/nuu1a6R1Xd0E0Mxs+fLic+9VXX0l5n3zyiVyzYcOGrmPeu6ZGRUVJNVetWiXP7490Zblv3z5r1qyZLViwwMzMjh07Zp07d7bo6Gjr27ev/e9//8uSgwGAW1WGzfLixYs2ZswYj6uKqVOnWnR0tH344YdWrlw5S0hIuKkHCQA5LcNmmS9fPps9e7aFh4enxxITE61p06Zm9scTRTZt2nTzjhAAbgEZfmaZN29ey5vXMy01NdXy5ctnZmbFixe3lJSUm3N0AHCLkJ9nOW3aNCtatKjFxMRYRERE+tXkkSNHbPDgwbZw4ULXv3v16lULCgrKmiMGgByQqbvhwcHBdunSJcufP78lJyd7/Iju5Ny5c47x0NBQ++233zxiN+Nu+Jdffinn/vOf/5Ty3O6whoSE+DzANpC74S1atJDybtbd8AsXLjjGk5OTrWTJkumvc/pu+I4dO+Sabufqp59+srJly3rErt/EzEhwcLA8/826G967d2+fmNMDjY8fPy7XvFXvhk+aNMn69+/vEbsZd8MnTJjgOpap37OsV6+erVy5Mv1AGjRokJkyAJBrZHhluXPnThs/frwlJSVZ3rx5beXKlTZx4kQbMmSIxcfH2z333GOtW7fOjmMFgByTYbOsXr26ffDBBz7xefPm3ZQDAoBbUbas4HHbhOngwYM+Y+pGZC+//LI8/+rVq+XcDz/8UMqrXbu2Yzw2NtaWLFniETtz5ow8/3fffSflPf7443LNnTt3Zsn8S5cuTf9zfHy8XNPpszU36teqQIECck1/m4t5j6kruLw3pfOnUaNGcu6KFSv+dF3vePny5eWabt/XTtTFKI899phc88qVK65jTz75pMdrdRXVhg0b5Pn9YW04AAholgAgoFkCgIBmCQACmiUACGiWACCgWQKAgGYJAAKaJQAIaJYAIMiW5Y7+HqfmPVarVi2pZpkyZeT5Bw4cKOeqS8PcNotyGrvjDv3/JHUJ2Q8//CDXPHHihJzr9titiIgIj7GHH35YrtmpUyc5V11uGMgS0oIFC7qO3fjYOTOzN954Q6p51113yfOXLl1azl23bp2c27hxY8d4SEiIx+vx48fLNdXlxmZmbdu2lfICeZyd264LTZs29Xkf6pLXQL5X/eHKEgAENEsAENAsAUBAswQAAc0SAAQ0SwAQ0CwBQECzBAABzRIABDRLABBky3LHixcvymPDhw+Xam7fvl2ef86cOXJunz59pLwHH3zQdeyhhx7yeK0uYTTTl6aNHj1arulvd0Nv/pamHTlyJP3PX3/9tVyzefPmcm6xYsWkvK1bt8o1v//+e8d4VFSUTZ061SN29uxZqeby5cvl+RcvXiznPvXUU3Luvffe6xh/9NFHPV4fOHBArjls2DA5NyUlRcrbtm2bXNPf0lTvsePHj0s1O3fuLM/vD1eWACCgWQKAgGYJAAKaJQAIaJYAIKBZAoCAZgkAApolAAholgAgyJYVPC+88II8dujQIanmjh075Pk7dOgg5w4YMEDKi46OdozXrl3b3n//fY/YmDFj5PnVFSQJCQlyzREjRsi5EydOdB27du1a+p/dVsU4+f333+VcdWVO9+7d5ZrLli1zHfNeXaSuzPnll1/k+evUqSPnpqWlybneG5O5xWNjY+Wae/bskXNDQ0OlvCZNmsg1/W3E5/19NGrUKKnm5s2b5fnbtWvnOsaVJQAIaJYAIKBZAoCAZgkAApolAAholgAgoFkCgIBmCQACmiUACGiWACDIkxbI+qpMKl++vGP88OHDPmPq0qjLly/L8zdq1EjOPXPmjJTntoQsLCzMZyOn+vXry/MPHDhQyrtw4YJcs1atWnJulSpVHOOlS5e2pKSk9NeBbBhWqlQpObdo0aJSnrqxm5n7uWrcuLGtXbvWI1aoUCGp5tixY+X5Azn/PXr0kHP/85//+MQ6duxoH330kUesZcuWcs1ActWvQbly5eSa48aNc4zPnj3bZ2n0jd+P/sybN0+ev2TJkq5jXFkCgEBqlvv27bNmzZrZggULzMxsyJAh9tRTT1nnzp2tc+fOPv87A8DtJsOnDl28eNHGjBljERERHvH+/ftbZGTkTTswALiVZHhlmS9fPps9e7aFh4dnx/EAwC1JvsEzbdo0K1q0qMXExNiQIUMsJSXFLl++bMWLF7eRI0dasWLFXP/u3r177f7778+ygwaA7Japh/+2atXKQkNDrVq1ajZr1iybPn263wdxPvHEE45x7ob74m44d8NV3A3PBXfDIyIirFq1amb2R3Pbt29fZsoAQK6RqWbZp08fO3r0qJmZJSYmWuXKlbP0oADgVpPhj+E7d+608ePHW1JSkuXNm9dWrlxpMTEx1q9fPytQoIAFBwe7XjoDwO0iw2ZZvXp1++CDD3zibp9DAsDtKFuWOz7zzDOO8UWLFvmMee+M6OaOO/RPEG7clTAjmzZtkvKCgoIc440aNbJ169Z5xAL5pf1Lly5JeefPn5drDh06VM5127EvODjYLl68mP46NTVVrhnI7o7eNyfctG7dWq7pdoOpYMGCPjfK1q9fL9VcuXKlPL9608rMfL53/GnRooVPbNCgQRYXF+cRK1GihFyzTZs2cm7Tpk2lvA0bNsg1z5075xgvVaqUHT9+3CN25coVqWbfvn3l+RcvXuw6xnJHABDQLAFAQLMEAAHNEgAENEsAENAsAUBAswQAAc0SAAQ0SwAQ0CwBQJCp51kGyt9yQ++xTz75RKr5wAMPyPO7LeFzoi6j/Pzzzx3jjRo18hlLTEyU53dbGuqtcOHCcs0333xTzu3Tp49jvHz58nbixIn01+qyTDOzmJgYOVd9TuUPP/wg13RbmtirVy977733PGKHDx+Wav7yyy/y/L1795ZzO3bsKOe6Lbl9+umnPV4Hstzws88+k3MrVaok5Z08eVKu2apVK8f4t99+67O80+3Zq94CWe7pD1eWACCgWQKAgGYJAAKaJQAIaJYAIKBZAoCAZgkAApolAAholgAgyJYVPP42rPIeq1mzplRzzZo18vyxsbFyboUKFaS8WbNmOcbHjRvnM9alSxd5/urVq0t58+fPl2uWK1dOzs2fP780NmTIELlmIKut6tSpI+V5bzTmj9v579Wrl8+KsenTp0s1t2zZIs/fvHlzOdd7szF/nHZdXbZsmfXv398j9s4778g1d+3aJeeqX6vRo0fLNStXriyPqccayAoqf7iyBAABzRIABDRLABDQLAFAQLMEAAHNEgAENEsAENAsAUBAswQAAc0SAATZstzxySeflMeCg4Olmjt37pTnd9uwyonbJlDe/G1Y5T3273//W54/LCxMyitYsGCW1zRz3zCuZ8+eHmMvvPCCXFNdFmdm9s0330h5hw4dkmtWq1ZNHtu7d69Uc/HixfL8S5YskXPT0tLk3Hnz5knxpKQkuWbbtm3l3B07dkh56iZ0Zv6//oMHD/Z4/fHHH0s1q1atKs/vD1eWACCgWQKAgGYJAAKaJQAIaJYAIKBZAoCAZgkAApolAAholgAgoFkCgCBbljtGR0fLYy+//LJU884775TnD2S5U7FixeRc1bJly+TcyMhIKa9Hjx5yza5du8q5w4cPdx0rXbp0+p8DWe7nvdugP3Xr1pXyrl27Jtf0t4Rv7NixHq/z5Mkj1UxJSZHn97e7qbeKFSvKuZs3b/aJtWzZ0ifeuHFjueawYcPkXHWHzbvuukuu6e/9e49t27ZNqvnII4/I8/sjNcu4uDjbvn27Xblyxbp37241atSwQYMG2dWrVy0sLMwmTJhg+fLly5IDAoBbUYbNcvPmzbZ//36Lj4+306dPW5s2bSwiIsKio6MtKirKJk2aZAkJCX6vHgEgt8vwM8s6derYlClTzMyscOHClpqaaomJida0aVMz++PHxk2bNt3cowSAHJZhswwKCkp/bFpCQoI1bNjQUlNT03/sLl68eECf3wBAbpQnTXyA3urVq23mzJk2d+5ca968efrV5JEjR2zw4MG2cOFC17979epV+TmRAHArkm7wbNiwwWbMmGFz5syxQoUKWXBwsF26dMny589vycnJFh4e7vfvnz9/3jFepEgRO3PmjEfsZtwNf/HFF+Vc9W6423u+6667fO5+/v3vf5fnf+utt6S81NRUuWZW3A3/29/+ZsuXL09//fnnn8s1O3bsKOeqd8O/+uoruabb3dDChQvb2bNnPWLq3XD1wbNmZtWrV5dzA7kbvnXrVp9Yy5YtbcWKFR6xQO6GX//ITREVFSXlPfDAA3JNt15RrFgx+/XXXz1i7dq1k2p2795dnr99+/auYxn+GH7u3DmLi4uzmTNnWmhoqJmZ1atXL/3p46tWrbIGDRrIBwMAuVGGV5YrVqyw06dPW79+/dJjb7zxho0YMcLi4+PtnnvusdatW9/UgwSAnJZhs2zfvr3jpanb/h8AcDvKlhU8GzdudIy3bNnSZ0z9fKdSpUry/P42rPJ26dIlKc9tVYbTZ5bz58+X5z937pyUN2DAALnm1KlT5Vx/n4UWLlw4/c9t2rSRa3p/Lu2P+psVgdT0/qzrusKFC/uMqZ+FN2nSRJ6/RIkScm7nzp3lXO/VR9eVLVvW4/XPP/8s1wzk96XVTeMC+f6vUaOGYzwiIsJnMzO1V2TVx4SsDQcAAc0SAAQ0SwAQ0CwBQECzBAABzRIABDRLABDQLAFAQLMEAAHNEgAE2bLc8YsvvnCMt2zZ0mcsKSlJqvnUU0/J8y9ZskTOvf5kpYycPn3aMd6xY0f79NNPPWJlypSR51+7dq2UF8ja/MGDB8u5PXv2dB3Lnz9/+p8zeizfjXbv3i3nbtmyRcqrXbu2XHPdunWO8fLly/sst92/f79UM5AN49QlrGZmo0ePlnNLlSolxT/44AO5prrc18ysT58+Ul4gj31z28vLabnjjd+P/gTyiER/j/7jyhIABDRLABDQLAFAQLMEAAHNEgAENEsAENAsAUBAswQAAc0SAAQ0SwAQ5ElLS0u72ZMcP37cMV6qVCmfsZCQEKnmqVOn5PkD2V1v586dUp7bjpGFCxe2s2fPesQCWW4VGxsr5Y0YMUKu+cADD8i5EydOdIxXrVrV9uzZk/5a/TqZBbYT4o1z+BPIjp1uy+IKFCjgs5vlypUrpZqbNm2S5+/Xr5+cO336dDl36NChPrGQkBA7f/68RywhIUGuuW3bNjlXVaVKFTnXbbnhxx9/7PPv6O2335ZqqkuIzczatWvnOsaVJQAIaJYAIKBZAoCAZgkAApolAAholgAgoFkCgIBmCQACmiUACLJlBU9MTIxjfMGCBT5jFy5ckGrWrVtXnn/GjBlyrroyZvjw4Y7x48eP+2wYVaRIEXn+sLAwKa9NmzZyzUA2d1u8eLFjfOjQoTZu3Lj01506dZJrLly4MMtze/fuLddctGiRY/yzzz6zqKgoj9izzz4r1SxdurQ8/7fffivnVq1aVc51WsGzdetWq1Onjkesa9eucs1WrVrJudeuXZPytm7dKtf88ssvHePTpk3z2SDNbWWgt4oVK8rzjx8/3nWMK0sAENAsAUBAswQAAc0SAAQ0SwAQ0CwBQECzBAABzRIABDRLABDQLAFAkDc7JunZs6c89q9//Uuq6b30yZ+2bdvKuW+99ZaU98orr8hjtWvXlud321zLm/eSSn+mTZsm5/r7Wt24NNBtYzMnvXr1knPV5a7bt2+Xa1aoUEEeO3z4sFRz6dKl8vzeyw/9KV68uJw7ZswYKb5q1Sq5ptvSUCcdOnSQ8t555x255sWLF13HduzY4fF68ODBUs3GjRvL8/sjNcu4uDjbvn27Xblyxbp3725r1qyxXbt2WWhoqJmZPffcc1l2QABwK8qwWW7evNn2799v8fHxdvr0aWvTpo3VrVvX+veWfGHtAAADSklEQVTvb5GRkdlxjACQ4zJslnXq1LGaNWua2R97YqemptrVq1dv+oEBwK0kwxs8QUFBFhwcbGZ/bNbesGFDCwoKsgULFlhsbKy99NJL9uuvv970AwWAnCQ/z3L16tU2c+ZMmzt3ru3cudNCQ0OtWrVqNmvWLDt+/LiNGjXK9e9euHDBChYsmGUHDQDZTbrBs2HDBpsxY4bNmTPHChUqZBEREeljTZo0sdGjR/v9+24PP61fv75t3LjRI6beDZ8zZ46UZ2aWlJQk56p3w6tXr+4Y79mzp8/dv9vhbnitWrU8zuPcuXPlmoHcDf/oo4+kvJMnT8o13a4H3n77bZ9jK1eunFQzkAf6BnI3PJD7AMnJyT6xFi1a2Oeff+4RC+RueNmyZeVc9W54586d5Zpud8M3btxo9evX94jdjLvhhQsXdh3L8Mfwc+fOWVxcnM2cOTP97nefPn3s6NGjZmaWmJholStXlg8GAHKjDK8sV6xYYadPn7Z+/fqlx9q2bWv9+vWzAgUKWHBwsMd2AwBwO8qwWbZv397at2/vEw9kDxgAyO1Y7ggAgmxZ7ui2u+OhQ4d8xtQbB/6WUHrz/mDYH/WufUpKSqbGMrJs2TIp79VXX5VrNm3aVM71dzPmxptK5cuXl2u2aNFCzj1z5oyUV6tWLbnm888/7zrm/b1x7NgxqWYgS/i6d+8u5wbyO8xONy6dbvBMmTJFrulvuaE3dclrQkKCXLNLly6uY947nx44cECqGcgSUn+9gitLABDQLAFAQLMEAAHNEgAENEsAENAsAUBAswQAAc0SAAQ0SwAQ5OgKHqex9evXSzWffvppef5AHtGkPvrrxIkTrmPNmzf3eH38+HF5fnW1h/p1MvP/2Clvf/nLXxzjx44d8xgL5BFxCxYskHPV93/vvffKNSdPnuwY79Spk8+Y9yoRN9OnT5fnX7t2rZwbFBQk527ZssUx3qBBA4/Xd955p1wzkI3g1PelPsrNzOz111+Xx9SVOQcPHpTn94crSwAQ0CwBQECzBAABzRIABDRLABDQLAFAQLMEAAHNEgAENEsAENAsAUCQJy0tLS2nDwIAbnVcWQKAgGYJAAKaJQAIaJYAIKBZAoCAZgkAgv8DYhlL8YXM8VMAAAAASUVORK5CYII=)"]},{"metadata":{"id":"6QSBTIpyx_Rk","colab_type":"text"},"cell_type":"markdown","source":["We, as humans, have a hard time recognizing anything on an image produced like this. Still, the Neural Network that we trained claims that it is more than 95% certain that the number shown on this image is a specific one.\n","\n","Part of the problem may be that we trained our image-classifier so that it always has to make up its mind. Could it help if we added artificial noise to the MNIST-images, and also some noise-only images, and added an 11th category: \"This is not a number, but noise?\" Developing this idea further, we would then likely want to traini a network to recognize whether a digit is human-generated or some \"fake\" data produced by an approach like the one above. How do we get \"fake\" digits? We could train another network for that, and pit the fake-digit-generating network against the fake-digit-recognizer of a digit-classifier so that they co-evolve. This is roughly the idea behind Generative Adversarial Networks\" .\n","\n","This exploration may serve as a warning: Deep-Learning based computer vision is actually somewhat different from human vision. With ML, computers may be good (often better than humans) to mostly do the right thing, statistically speaking. But this can come with some occasional very strange behavior. Deep Learning as it is implemented in this toy example is good at finding some correlations in the input data which allow making predictions that are correct with high probability. In Engineering applications, we need to be very careful to not just make assumptions, ignoring that they may be violated in practice. Many ML applications are of the form \"Let us find some mapping from many numbers (such as for example: image data) to (for example) some probability\". The trained model will map any input to some output. If it only ever saw a tiny part of all theoretically possible inputs in training, we must not expect it to show desirable behavior when encountering data that is very different from training examples. Even when restricting ourselves to some kind of \"plausible input subspace\", such as (for example) all images that we can obtain from interpolating between known training examples, we can expect that with a bit of effort, it is possible to construct input examples where the model's behavior diverges strongly from our expectations (since the model is only doing reasonably well statistically, not necessarily for individual points - especially if we are looking for such points). Knowing to ask the right questions here is not only important for ML practitioners, but just as well for decision makers, which in democratic societies in some situations means: the population."]},{"metadata":{"id":"DcHGxjRFWqEX","colab_type":"text"},"cell_type":"markdown","source":["Exercises\n","*   The mod-9 scheme has some basic problems.\n","     Can it find all one-digit-recognition-errors? Would it help to instead\n","     take multiples of 11 as valid account-numbers. There is a simple rule\n","     to check whether a number is divisible by 11. Change the code to consider\n","     numbers divisible by 11 as valid account numbers.\n","*   Simplify the code for the MNIST model by replacing tf.keras.Model \n","    with tf.Keras.Sequential.\n","\n","Homework\n","\n","*   Do mod-11 with and without using tf.einsum().\n","*   Try make the account-number model self-contained, \n","     i.e. rather than referring to a MNIST-model, there should be layers for \n","     digit-recognition inside it, but each of the four digit-recognition \n","     sub-problems should use the same layers (and hence weights).\n","*   Convert from TF-Eager to non-eager tensorflow code.\n"]},{"metadata":{"id":"Ft7KISUEBEZ1","colab_type":"code","colab":{}},"cell_type":"code","source":["# SOLUTIONS\n","\n","# --- Mod-11 checksum ---\n","\n","mod11sum = numpy.zeros([10, 10, 11],  # Note tensor shape!\n","                       dtype=numpy.float32)\n","for n1 in xrange(10):\n","  for n2 in xrange(10):\n","    mod11sum[n1, n2, (n1 + n2) % 11] = 1.0\n","\n","# The corresponding tensor-constant:\n","tc_mod11sum = tf.constant(mod11sum)\n","\n","# Index 'b' is the batch-example index. Number of batch-examples is 'B'.\n","def get_prob_for_being_multiple_of_11(logit_by_digit_and_place, debug=True):\n","  # The parameter 'logit_by_digit_and_place' is a (B, 10, 4)-tensor.\n","  # Entry [b, 7, 0] is the predicted logit-space evidence for the 4-digit\n","  # number to have a '7' in the 'thousands' place, for example 'b'.\n","  # Entry [9, 1] is the predicted logit-space evidence for the 4-digit\n","  # number to have a '9' in the 'hundreds' place, for example 'b'.\n","  #\n","  # We first map logits to probabilities with softmax().\n","  probability_by_digit_and_place = tf.nn.softmax(logit_by_digit_and_place)\n","  # Index-names correspond to roman-numeral place-values.\n","  #\n","  #\n","  # Here and below: We use latin-inspired index-names M, C, X, I for thousands,\n","  # hundreds, tens, and ones. The 'tf.einsum()' tensor-product can be read as an \n","  # equivalent SQL statement roughly of the form:\n","  #\n","  # select M.NumExample, M.Digit, C.Digit, M.Value * C.Value\n","  # from ExamplesThousands as M, ExamplesHundreds as C\n","  # where M.NumExample = C.NumExample;\n","  #\n","  # (B, 10, 10)-tensor.\n","  # Entry [b, 2, 3] is the probability for the number to be of the form 2x3x.\n","  prob_digits13 = tf.einsum('bM,bC->bMC',\n","                            probability_by_digit_and_place[:, 0],\n","                            probability_by_digit_and_place[:, 2])\n","  # (B, 11)-tensor.\n","  # Entry [b, 7] is the probability for the sum-mod-11 of the Thousands and \n","  # Tens-digits to be 7.\n","  mod11prob_digits13 = tf.einsum('MCr,bMC->br', tc_mod11sum, prob_digits13)\n","  if debug:\n","    print(\"DEBUG: mod11prob digits M+X\", mod11prob_digits13.numpy())\n","  # Likewise: Digits 2 and 4.\n","  prob_digits24 = tf.einsum('bX,bI->bXI',\n","                            probability_by_digit_and_place[:, 1],\n","                            probability_by_digit_and_place[:, 3])\n","  mod11prob_digits24 = tf.einsum('XIs,bXI->bs', tc_mod11sum, prob_digits24)\n","  if debug:\n","    print(\"DEBUG: mod11prob digits C+I\", mod11prob_digits24.numpy())\n","  # (B, 11, 11)-tensor. Entry [b, 4, 7] is the probability for the first\n","  # two digits to have mod-11 sum 4 and the last two digits to have\n","  # mod-11 sum 7.\n","  mod11prob_digits13_24 = tf.einsum('br,bs->brs',\n","                                     mod11prob_digits13,\n","                                     mod11prob_digits24)\n","  if debug:\n","    debug_probs_example0 = mod11prob_digits13_24.numpy()[0]\n","    for n1 in xrange(11):\n","      for n2 in xrange(11):\n","        prob_n1n2 = debug_probs_example0[n1, n2]\n","        if prob_n1n2 > 0.01:\n","          print(\"{:02d}, {:02d}: {:.2f}\".format(n1, n2, prob_n1n2))\n","  # We are interested in the probability of both mod-11 sums to be equal.\n","  # This is a batched matrix-trace operation.\n","  #\n","  ret = tf.einsum('brr->b', mod11prob_digits13_24)  # Note.\n","  if debug:\n","    print(ret)\n","  return ret\n","\n","\n","# Some artificial logits that correspond to a 4-digit number that most likely\n","# would read as '2145'.\n","t_ddd_ex2145 = tf.constant(numpy.array([\n","    [-1.0, -1.0,  9.0, -1.0, -1.0,  -1.0, -1.0, -1.0, -1.0, -1.0],\n","    [-1.0,  9.0, -1.0, -1.0, -1.0,  -1.0, -1.0, -1.0, -1.0, -1.0],\n","    [-1.0, -1.0, -1.0, -1.0,  9.0,  -1.0, -1.0, -1.0, -1.0, -1.0],\n","    [-1.0, -1.0, -1.0, -1.0, -1.0,   9.0, -1.0, -1.0, -1.0, -1.0]],\n","    dtype=numpy.float32))\n","\n","print('Probability ~2145',\n","      get_prob_for_being_multiple_of_11(tf.reshape(t_ddd_ex2145, (1, 4, 10))))\n","\n","# --- tf.keras.Sequential model ---\n","mnist_model_seq = tf.keras.Sequential([\n","  tf.keras.layers.Dense(\n","        units=120,\n","        activation=tf.keras.activations.relu,\n","        kernel_regularizer=tf.keras.regularizers.l2(0.003)),\n","  tf.keras.layers.Dense(\n","        units=40,\n","        activation=tf.keras.activations.relu,\n","        kernel_regularizer=tf.keras.regularizers.l2(0.003)),\n","  tf.keras.layers.Dense(units=10)])\n","\n","debug_eval_seq0 = mnist_model_seq(debug_batch0)\n","# Logits are perhaps not as telling as probabilities, so we map logits\n","# to actual probabilities with tf.nn.softmax(). As the model's initial weights\n","# are essentially random, we expect these probabilities to be very roughly\n","# similar, and not meaningful.\n","print('Example Model Evaluation:', tf.nn.softmax(debug_eval_seq0))\n"],"execution_count":0,"outputs":[]}]}